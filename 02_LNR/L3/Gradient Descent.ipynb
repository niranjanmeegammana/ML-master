{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rGGH28a2OYv8"},"source":["## **Gradient Descent**\n","\n","Why should I care about Gradient Descent?\n","\n","\n","*   It's used in pretty much every machine learning algorithm\n","*   It's the process by which the machine \"learns\"\n","\n","Lets dive down (literally) and recall some derivative stuff from last session. . . \n"]},{"cell_type":"markdown","metadata":{"id":"R-sqongUQMV_"},"source":["## **Derivatives -> Gradient Descent**\n","\n","Recall that derivatives are a way to describe a function's **rate of change.**\n","\n","Taking the derivative of a function results in another function, which can then be used to calculate the rate of change at that point in the function:\n","\n",">$f(x) = (x-1)^2$\n","\n",">$\\frac{\\delta f}{\\delta x} = f'(x) = 2(x - 1) = 2x - 2 $\n","\n","![alt text](https://eli.thegreenplace.net/images/2016/plot-parabola-with-tangents.png)\n","\n","**Gradient descent** is an algorithm for getting to the bottom, or minimum, of a convex function. In our case of machine learning, the $y$ axis is the amount of ERROR, and we want to go low to the bottom of the valley to get minimal ERROR. \n","\n","A function is convex if the line segment between any two points on the graph of the function lies above or on the graph. Therefore, $x^2$ is convex (the function opens upwards). It has a minimum at $x_{min} = 1$.\n","\n","Now, let's discuss gradient descent at a high level, using skiing down a hill as an analogy!\n","\n","Let's consider $f(x) = x^2$. We know that $\\frac{\\delta f}{\\delta x} = 2x$, and that $f(x)$ has $x_{min} = 0$. Remember, gradient descent is all about getting to a minimum! You want to get low :)\n","\n","It seems that you should use $f'(x)$ as a pair of skis to ski down the slope of $f(x)$. Obviously, if you're skiing down a steeper part of $f(x)$, then you would move faster.\n","\n","**Gradient descent:**\n","\n","```while``` $x_{n+1} \\neq x_n$ ```:```\n","\n","> $x_{n+1} = x_n - \\alpha * \\frac{\\delta f}{\\delta x}|_{x=x_n}$\n","\n","I know, this formula looks scary. Don't worry, we're going to take a second now (actually, more like a minute) to discuss the notation.\n","\n","1. $\\alpha$ is called a hyperparameter and YOU set its value. It controls how fast you ski down $f(x)$, that is, the larger $\\alpha$, the more ground you cover. But, be careful! If $\\alpha$ is set too high, you might fly off the function.\n","2. $\\lim_{n \\to \\infty} x_n = x_{min}$\n","\n","\n",">Let's see if gradient descent does what we want, using $f(x)$ as a test! We can first set $\\alpha$ to 0.25.\n","\n","$x_{n+1} = x_n - 0.25 * 2 x_n = x_n - 0.5 x_n$\n","\n","Let's start at $x_0 = 2$.\n","\n","$x_1 = x_0 - 0.5 x_0 = 2 - 0.5 * 2 = 2 - 1 = 1$\n","\n","$x_2 = x_1 - 0.5 x_1 = 1 - 0.5 * 1 = 1 - 0.5 = 0.5$\n","\n","$x_3 = x_2 - 0.5 x_2 = 0.5 - 0.5 * 0.5 = 0.5 - 0.25 = 0.25$\n","\n","$x_4 = x_3 - 0.5 x_3 = 0.25 - 0.5 * 0.25 = 0.25 - 0.125 = 0.125$\n","\n","We can clearly see that $x_{min} = \\lim_{n \\to \\infty} x_n = 0$. You might be wondering: wait, $x_n$ gets really close to 0, but it never actually gets there... won't a computer running gradient descent get stuck in an inifinite loop?? Nope! Eventually, $x_n$ will get SO close to 0 that a computer won't be able to tell the difference between $x_n$ and 0 due to its floating point precision. The gradient descent algorithm terminates when $x_n$ stops changing.\n","\n",">Now, let's set $\\alpha$ to 0.5.\n","\n","$x_{n+1} = x_n - 0.5 * 2 x_n = x_n - x_n$\n","\n","Let's start at $x_0 = 2$.\n","\n","$x_1 = x_0 - x_0 = 0$\n","\n","$x_2 = x_1 - x_1 = 0$\n","\n","The gradient descent algorithm terminates when $x_n$ stops changing. Indeed, $x_{min} = x_2 = 0$!\n","\n",">Now, let's set $\\alpha$ to 2.\n","\n","$x_{n+1} = x_n - 2 * 2 x_n = x_n - 4 x_n$\n","\n","Let's start at $x_0 = 2$.\n","\n","$x_1 = x_0 - 4 * x_0 = 2 - 4 * 2 = -6$\n","\n","$x_2 = x_1 - 4 * x_1 = -6 - 4 * (-6) = 18$\n","\n","$x_3 = x_2 - 4 * x_2 = 18 - 4 * (18) = -54$\n","\n","Oof, $x_n$ doesn't seem to be getting closer to 0 at all. In fact, it seems to be diverging!\n","\n",">Now, let's set $f(x) = -x^2$ and $\\alpha$ to 0.5 (which worked before on $f(x) = x^2$).\n","\n","$x_{n+1} = x_n - 0.5 * (-2 x_n) = x_n + x_n$\n","\n","Let's start at $x_0 = 2$.\n","\n","$x_1 = x_0 + x_0 = 2 + 2 = 4$\n","\n","$x_2 = x_1 + x_1 = 4 + 4 = 8$\n","\n","$x_3 = x_2 + x_2 = 8 + 8 = 16$\n","\n","Oh no, $x_n$ doesn't seem to be getting closer to 0 at all. It seems to be diverging again!\n","\n","\\\n","It seems that the gradient descent algorithm works, but only when 2 conditions are met:\n","\n","1. The learning rate is set appropriately. In practice, the learning rate needs to be experimented with until gradient descent performs the best. Also, you should always set $0 \\leq \\alpha \\leq 1$. Lastly, as seen from the last example, a learning rate may work well on one function but not on another.\n","2. The function is convex. $f(x) = -x^2$ is NOT convex. So, gradient descent will try to push $x_n$ all the way to $+\\infty$ or $-\\infty$ (depending on the sign of $x_0$). On a computer, this will result in an INFINITE LOOP, since $x_n$ will NEVER stop changing."]},{"cell_type":"markdown","metadata":{"id":"VzgWcpNVBs-U"},"source":["## **Is Gradient Descent possible with multiple variables?**\n","\n","Most certainly! But before we go forward, let's recall what a partial derivative is:\n","\n","**Partial Derivative** is taking the derivative of a function with respect to only one variable (a.k.a. taking the derivative one variable at a time). See the ML Math collab for more examples.\n","<br>\n","\n","> So back to the question, how would we extend gradient descent to a multivariate function?\n","\n","We can just run gradient descent on *each* input variable (using the appropriate partial derivative) simultaneously.\n","\n","So, if we have $f(x, y)$, on each iteration of gradient descent we would do: \n","\n","$x_{n+1} = x_n - \\alpha * \\frac{\\delta f}{\\delta x}|_{x=x_n}$\n","\n","$y_{n+1} = y_n - \\alpha * \\frac{\\delta f}{\\delta y}|_{y=y_n}$\n","\n","We would stop when BOTH $x_{n+1} = x_n$ AND $y_{n+1} = y_n$. Why might it be an issue if we stop when only one input variable stops changing?\n","\n","***Food for thought:*** before, we were skiing down our function, now, we're sledding down our function (using a hyperplane)\n"]},{"cell_type":"markdown","metadata":{"id":"9a3WUSZ-QwsL"},"source":["## **Vectors, Matrices, and Gradient Descent**\n","\n","**The big question: how can we use vectors and matrices to formalize function definitions and gradient descent?**\n","\n","Remember what it means to vectorize a function:\n","\n","We can use *gradients* to formalize functions for gradient descent. Meaning, finding the gradient is the key to finding the slope in all directions $x$-$y$-$z$ on our hyperplane.\n","\n","> $b(x, y, z) = xyz$\n","\n","> $\\nabla b = \\begin{pmatrix}\n","           \\frac{\\delta b}{\\delta x} \\\\\n","           \\frac{\\delta b}{\\delta y} \\\\\n","           \\frac{\\delta b}{\\delta z} \n","         \\end{pmatrix} $\n","\n","\\\\\n","Now we have a vector that stores the 3D slope which we can ride down!\n","\n","So given our current understanding of gradient descent, for the function $b(x, y, z)$, we would do the following on each iteration:\n","\n","$x_{n+1} = x_n - \\alpha * \\frac{\\delta b}{\\delta x}|_{x=x_n}$\n","\n","$y_{n+1} = y_n - \\alpha * \\frac{\\delta b}{\\delta y}|_{y=y_n}$\n","\n","$z_{n+1} = z_n - \\alpha * \\frac{\\delta b}{\\delta z}|_{z=y_n}$\n","\n","But, we could also do:\n","\n","$\\begin{pmatrix}\n","           x_{n+1} \\\\\n","           y_{n+1} \\\\\n","           z_{n+1} \n","         \\end{pmatrix} = \\begin{pmatrix}\n","           x_{n} \\\\\n","           y_{n} \\\\\n","           z_{n} \n","         \\end{pmatrix} - \\alpha \\nabla b |_{(x=x_n, y = y_n, z=z_n)}$\n","\n","until ALL the input variables stop changing.\n","\n","Take some time to convince yourself of this!\n","\n","Ok, cool, so now, even when we have multiple variables in our function input, we can still use gradient descent!\n","\n","Let's retroactively discuss some of the intuition behind gradient descent. The gradient is actually a vector that points in the direction of greatest **initial** increase no matter at which point you are on a function. (I say **initial** because a function's shape may change very quickly.) So, you could follow the gradient to get to the maximum of a function. Thus, it makes sense to follow the negative gradient to get the minimum of a function, since the negative gradient points in the direction of greatest **initial** decrease.\n","\n","You might still be wondering what I mean by **initial**. Let's say you're at the top of a 2D hill, and you're trying to ski down the shortest route to the bottom of the hill (look at the diagram below). You have to pick which side of the hill to ski down. The gradient at EXACTLY the top of the hill is just $<0>$ (this is a one-entry vector, and this makes sense because there's only one input variable). So, you experiment! You move to the left a little and see what the gradient is like there... you find that it's steep (which direction does the gradient point? towards the top of the hill). Now, you move to the right side of the hill... and you find that the gradient is even steeper (which direction does the gradient point? towards the top of the hill), so you follow the negative gradient down the right side. Oh crap, it turns out that going down the right side leads you to the bottom of a second, really steep hill (where the gradient is massive) and NOT the bottom of the original hill. Whereas, going down the left side of the hill would have taken you to the base of the original hill.\n","\n","In summary, the gradient tells you about the direction of greatest increase of the function ONLY at the point at which you are standing; you won't know the slope of the function for sure anywhere else. That's why gradient descented is only guaranteed to take the shortest route to the minimum of a function if the function is convex (i.e. doesn't look like these hills).![alt text](https://drive.google.com/uc?id=1NmNUhMp6XvqtmhSBOfFLIstjayGv7-iJ)\n","\n","One last thing, we said gradient descent terminates when the input variables stop changing. This occurs when the gradient is the 0-vector. The gradient is only the 0-vector at maxima and minima and saddle points. So, this provides more evidence that gradient descent can actually take us to a minimum of a function. Note that I said **a** minimum, NOT **the** global minimum. This is **IMPORTANT**: gradient descent is not guaranteed to find a global minimum if the function is not convex, it can get stuck at local minima and saddle points. But, we trust it enough in machine learning to do the job even when our functions are not completely convex!\n"]}]}